{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a145f301",
   "metadata": {},
   "source": [
    " # WWH BDT Training Notebook\n",
    " \n",
    " \n",
    " \n",
    " ## Importing and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6960966a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import uproot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_curve\n",
    "import json\n",
    "import tqdm\n",
    "import math\n",
    "import os\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import bdt_drawing as bdraw\n",
    "import bdt_training as btrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63cb925",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "branches_tmp = [\"Sample_name\",\n",
    "                \"genWeight\",\n",
    "                \"Xsec_genWeight\",\n",
    "                \"EventType_SemiLeptonic\",\n",
    "                \"EventType_Leptonic\",\n",
    "                \"CandidateVBF_Lead_Jet_pt\",\n",
    "                \"CandidateVBF_Trail_Jet_pt\",\n",
    "                \"CandidateVBF_Jet_invMass\",\n",
    "                \"CandidateVBF_Jet_etaSep\",\n",
    "                \"CandidateHiggs_FatJet_pt\",\n",
    "                \"CandidateHiggs_FatJet_eta\",\n",
    "                \"CandidateHiggs_FatJet_msoftdrop\",\n",
    "                \"CandidateHiggs_FatJet_mass\",\n",
    "                \"CandidateHiggs_FatJet_particlenetScore\",\n",
    "                \"CandidateW_SemiLeptonic_FatJet_pt\",\n",
    "                \"CandidateW_SemiLeptonic_FatJet_eta\",\n",
    "                \"CandidateW_SemiLeptonic_FatJet_msoftdrop\",\n",
    "                \"CandidateW_SemiLeptonic_FatJet_mass\",\n",
    "                \"CandidateW_SemiLeptonic_FatJet_particlenetScore\",\n",
    "                \"CandidateW_SemiLeptonic_FatJet_flavor\",\n",
    "                \"Candidate_SemiLeptonic_Lepton_pt\",\n",
    "                \"Candidate_SemiLeptonic_Lepton_eta\",\n",
    "                \"MET_pt\",\n",
    "                \"Candidate_Leptonic_Lead_Lepton_pt\",\n",
    "                \"Candidate_Leptonic_Trail_Lepton_pt\",\n",
    "                \"Candidate_Leptonic_Lead_Lepton_eta\",\n",
    "                \"Candidate_Leptonic_Trail_Lepton_eta\",\n",
    "                \"Candidate_Leptonic_Lead_Lepton_phi\",\n",
    "                \"Candidate_Leptonic_Trail_Lepton_phi\",\n",
    "                \"Candidate_SemiLeptonic_ST\",\n",
    "                \"Candidate_Leptonic_ST\",\n",
    "                \"Candidate_Leptonic_LT\",\n",
    "                \"Candidate_SemiLeptonic_LT\",\n",
    "                \"Candidate_Leptonic_MT\",\n",
    "                \"Candidate_SemiLeptonic_MT\",\n",
    "                \"Candidate_SemiLeptonic_RpT\",\n",
    "                \"Candidate_Leptonic_RpT\",\n",
    "                #\"FatJet_particleNet_WvsQCD\",\n",
    "                #\"CandidateW_SemiLeptonic_FatJet_idx\"\n",
    "                \"Candidate_Leptonic_Lead_Lepton_type\",\n",
    "                \"Candidate_Leptonic_Trail_Lepton_type\",\n",
    "                \"Candidate_SemiLeptonic_Lepton_type\",\n",
    "                \"Candidate_Leptonic_Lepton_InvMass\"\n",
    "               ]\n",
    "\n",
    "eos_path = \"/eos/cms/store/user/lzygala/HVV/Selection_TTrees_v2/AllYears/BDT_Settings_01232023/\"\n",
    "\n",
    "display_columns = [\"name\",\"year\",\"xsec\",\"nRawEvents\",\"nScaledEvents\",\"included\"]\n",
    "df_displayinfo_l = pd.DataFrame(columns = display_columns)\n",
    "df_displayinfo_s = pd.DataFrame(columns = display_columns)\n",
    "\n",
    "dfs_l = []\n",
    "dfs_s = []\n",
    "\n",
    "dfs_sig_l = {}\n",
    "dfs_sig_s = {}\n",
    "\n",
    "cut_l = \"CandidateVBF_Jet_etaSep >= 4 and Candidate_Leptonic_ST > 1000\"\n",
    "cut_s = \"CandidateVBF_Jet_etaSep >= 4 and Candidate_SemiLeptonic_ST > 1000\"\n",
    "\n",
    "years = [\"16APV\", \"16\", \"17\", \"18\"]\n",
    "lumis = {\n",
    "    \"16APV\" : 19.52, \n",
    "    \"16\" : 16.81, \n",
    "    \"17\" : 41.48, \n",
    "    \"18\" : 59.83\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a9bcc2",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "All events are scaled by genWeight * xs * lumi / sum of genWeights\n",
    "- Run 2 luminosity = 137.65\n",
    "\n",
    "Single signal file is expected\n",
    "\n",
    "### Load signal file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e674d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_types = [\"C2V_0\"\n",
    "               ]\n",
    "signal_xs = {\"WWH_ucsd_C2V_Reweight\" : 0.00243354,\n",
    "             \"WZH_ucsd_C2V_Reweight\" : 0.00155167\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3609fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_file_pre = \"SIG_WWH_WZH_\"\n",
    "bkg_file_pre = \"BKG_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c9ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    f_signal = eos_path + sig_file_pre + year + \".root\"\n",
    "    print(f_signal)\n",
    "    \n",
    "    f_open_signal_l = uproot.open(f_signal+\":Events_leptonic\")\n",
    "    f_open_signal_s = uproot.open(f_signal+\":Events_semileptonic\")\n",
    "    f_open_signal_cutflow = uproot.open(f_signal+\":CutFlow\")\n",
    "\n",
    "    df_signal_s = f_open_signal_s.arrays(branches_tmp,library=\"pd\")\n",
    "    df_signal_l = f_open_signal_l.arrays(branches_tmp,library=\"pd\")\n",
    "    \n",
    "    df_signal_s[\"sig_type\"] = \"C2V_Reweight\"#signal_sample[9:]\n",
    "    df_signal_s[\"abs_xsec_genWeight\"] = np.abs(df_signal_s[\"Xsec_genWeight\"])\n",
    "    df_signal_s[\"sample_isSignal\"] = True\n",
    "    df_signal_s[\"year\"] = year\n",
    "\n",
    "    df_signal_l[\"sig_type\"] = \"C2V_Reweight\"#signal_sample[9:]\n",
    "    df_signal_l[\"abs_xsec_genWeight\"] = np.abs(df_signal_l[\"Xsec_genWeight\"])\n",
    "    df_signal_l[\"sample_isSignal\"] = True\n",
    "    df_signal_l[\"year\"] = year\n",
    "\n",
    "    #df_displayinfo_l.loc[len(df_displayinfo_l.index)] = [signal_sample, year, signal_xs[signal_sample], len(df_signal_l), df_signal_l[\"abs_xsec_genWeight\"].sum(),True]\n",
    "    #df_displayinfo_s.loc[len(df_displayinfo_s.index)] = [signal_sample, year, signal_xs[signal_sample], len(df_signal_s), df_signal_s[\"abs_xsec_genWeight\"].sum(),True]\n",
    "\n",
    "    df_signal_s = df_signal_s[df_signal_s.eval(cut_s)].copy()\n",
    "    df_signal_l = df_signal_l[df_signal_l.eval(cut_l)].copy()\n",
    "\n",
    "    dfs_l.append(df_signal_l)\n",
    "    dfs_s.append(df_signal_s)\n",
    "\n",
    "    #dfs_sig_l[signal_sample].append(df_signal_l)\n",
    "    #dfs_sig_s[signal_sample].append(df_signal_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68623787",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dfs_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ff997c",
   "metadata": {},
   "source": [
    "### Load background files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ff772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info_backgrounds = json.load(open(\"bkg_info.json\"))\n",
    "\n",
    "dfs_l_background = []\n",
    "dfs_s_background = []\n",
    "\n",
    "for year in years:\n",
    "    #print(sample[\"name\"], year)\n",
    "    filename = eos_path + bkg_file_pre + year +\".root\"\n",
    "    print(filename)\n",
    "\n",
    "    f_open_background_s = uproot.open(filename+\":Events_semileptonic\")\n",
    "    f_open_background_l = uproot.open(filename+\":Events_leptonic\")\n",
    "    f_open_background_cutflow = uproot.open(filename+\":CutFlow\")\n",
    "\n",
    "    df_background_s = f_open_background_s.arrays(branches_tmp,library=\"pd\")\n",
    "    df_background_l = f_open_background_l.arrays(branches_tmp,library=\"pd\")\n",
    "\n",
    "    df_background_s = df_background_s[df_background_s.eval(cut_s)].copy()\n",
    "    df_background_l = df_background_l[df_background_l.eval(cut_l)].copy()\n",
    "\n",
    "    df_background_s[\"sig_type\"] = \"bkg\"\n",
    "    df_background_s[\"abs_xsec_genWeight\"] = np.abs(df_background_s[\"Xsec_genWeight\"])\n",
    "    df_background_s[\"sample_isSignal\"] = False\n",
    "    df_background_s[\"year\"] = year\n",
    "\n",
    "    df_background_l[\"sig_type\"] = \"bkg\"\n",
    "    df_background_l[\"abs_xsec_genWeight\"] = np.abs(df_background_l[\"Xsec_genWeight\"])\n",
    "    df_background_l[\"sample_isSignal\"] = False\n",
    "    df_background_l[\"year\"] = year\n",
    "\n",
    "    if df_background_l.empty == False:\n",
    "        dfs_l.append(df_background_l)\n",
    "        dfs_l_background.append(df_background_l)\n",
    "        #df_displayinfo_l.loc[len(df_displayinfo_l.index)] = [sample[\"name\"], year,  sample[\"xs\"], len(df_background_l), df_background_l[\"abs_xsec_genWeight\"].sum(),True]\n",
    "\n",
    "    if df_background_s.empty == False:\n",
    "        dfs_s.append(df_background_s)\n",
    "        dfs_s_background.append(df_background_s)\n",
    "        #df_displayinfo_s.loc[len(df_displayinfo_s.index)] = [sample[\"name\"],year,  sample[\"xs\"], len(df_background_s), df_background_s[\"abs_xsec_genWeight\"].sum(),True]\n",
    "\n",
    "display(df_displayinfo_l)\n",
    "display(df_displayinfo_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277e6af7",
   "metadata": {},
   "source": [
    "## Split Samples for Training and Testing\n",
    "\n",
    "Each sample is split:\n",
    "- 75% of raw events for training\n",
    "- 25% of raw events for testing\n",
    "\n",
    "The training and testing sets then have their weights rescaled so each set has an integral equal to the total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a893be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_samples(df, use_abs=True):\n",
    "    split_columns = [\"name\",\"year\",\"total_integral\",\"test_integral\",\"train_integral\"]\n",
    "    split_columns_info = [\"name\",\"year\",\"total_integral\",\"test_integral\",\"train_integral\"]\n",
    "    df_split_info = pd.DataFrame(columns = split_columns)\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    for name in df.Sample_name.unique():\n",
    "        for year in df.year.unique():\n",
    "            df_tmp = df[(df.year==year) & (df.Sample_name == name)].copy()\n",
    "            \n",
    "            if df_tmp.empty == True:\n",
    "                continue\n",
    "            df_tmp[\"split_train\"] = (np.random.rand(len(df_tmp)) < 0.75)\n",
    "            split_train = df_tmp[\"split_train\"]\n",
    "            split_test = ~df_tmp[\"split_train\"]\n",
    "\n",
    "            total, split_test_integral, split_train_integral = 0, 0, 0\n",
    "\n",
    "            if use_abs:\n",
    "                df_tmp[\"split_weight\"] = df_tmp.abs_xsec_genWeight\n",
    "                total = df_tmp.abs_xsec_genWeight.sum()\n",
    "                split_test_integral = df_tmp[split_test].abs_xsec_genWeight.sum()\n",
    "                split_train_integral = df_tmp[split_train].abs_xsec_genWeight.sum()\n",
    "            else:\n",
    "                df_tmp[\"split_weight\"] = df_tmp.xsec_genWeight\n",
    "                total = df_tmp.Xsec_genWeight.sum()\n",
    "                split_test_integral = df_tmp[split_test].Xsec_genWeight.sum()\n",
    "                split_train_integral = df_tmp[split_train].Xsec_genWeight.sum()\n",
    "\n",
    "            df_split_info.loc[len(df_split_info.index)] = [name, year, total, split_test_integral, split_train_integral]\n",
    "\n",
    "\n",
    "            if split_train_integral != 0 and split_test_integral != 0:\n",
    "                df_tmp.loc[split_train, \"split_weight\"] *= total/split_train_integral\n",
    "                df_tmp.loc[split_test, \"split_weight\"] *= total/split_test_integral\n",
    "            else:\n",
    "                #print(df.iloc[-1].at[\"sample_name\"]+\" 0 occured!\")\n",
    "                #display(df[[\"split_train\"]])\n",
    "                df_tmp.loc[split_train, \"split_weight\"] *= 0\n",
    "                df_tmp.loc[split_test, \"split_weight\"] *= 0\n",
    "            \n",
    "            dfs.append(df_tmp)\n",
    "\n",
    "    df_combined = pd.concat(dfs)\n",
    "    display(df_split_info)\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999910d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_s_tmp = pd.concat(dfs_s)\n",
    "df_l_tmp = pd.concat(dfs_l)\n",
    "\n",
    "df_combined_s_LepType = split_samples(df_s_tmp)\n",
    "df_combined_l_LepType = split_samples(df_l_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7024742",
   "metadata": {},
   "source": [
    "## Set Up Model and Train\n",
    "\n",
    "### Variables to be trained on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfed503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_combined_l_LepType[\"Lep_dPhi\"] = np.abs(df_combined_l_LepType[\"Candidate_Leptonic_Lead_Lepton_phi\"] - df_combined_l_LepType[\"Candidate_Leptonic_Trail_Lepton_phi\"])\n",
    "df_combined_l_LepType.loc[df_combined_l_LepType.Lep_dPhi > 3.14159, \"Lep_dPhi\"] = df_combined_l_LepType.loc[df_combined_l_LepType.Lep_dPhi > 3.14159, \"Lep_dPhi\"] - (2*3.14159)\n",
    "\n",
    "df_combined_l_LepType[\"Lep_dEta\"] = np.abs(df_combined_l_LepType[\"Candidate_Leptonic_Lead_Lepton_eta\"] - df_combined_l_LepType[\"Candidate_Leptonic_Trail_Lepton_eta\"])\n",
    "df_combined_l_LepType[\"Lep_dR\"] = np.sqrt(np.power(df_combined_l_LepType[\"Lep_dPhi\"], 2) + np.power(df_combined_l_LepType[\"Lep_dEta\"],2))\n",
    "\n",
    "features_s_LepType = {\n",
    "    \"CandidateVBF_Jet_invMass\" : \"f0\",\n",
    "    \"CandidateVBF_Jet_etaSep\" : \"f1\",\n",
    "    \"CandidateVBF_Lead_Jet_pt\" : \"f2\",\n",
    "    \"CandidateVBF_Trail_Jet_pt\" : \"f3\",\n",
    "    \"CandidateHiggs_FatJet_pt\" : \"f4\",\n",
    "    \"CandidateHiggs_FatJet_eta\" : \"f5\",\n",
    "    \"CandidateHiggs_FatJet_msoftdrop\" : \"f6\",\n",
    "    #\"CandidateHiggs_FatJet_particlenetScore\",\n",
    "    \"CandidateW_SemiLeptonic_FatJet_pt\" : \"f7\",\n",
    "    \"CandidateW_SemiLeptonic_FatJet_eta\" : \"f8\",\n",
    "    \"CandidateW_SemiLeptonic_FatJet_msoftdrop\" : \"f9\",\n",
    "    \"CandidateW_SemiLeptonic_FatJet_flavor\" : \"f10\",\n",
    "    \"Candidate_SemiLeptonic_Lepton_pt\" : \"f11\",\n",
    "    \"Candidate_SemiLeptonic_Lepton_eta\" : \"f12\",\n",
    "    \"MET_pt\" : \"f13\",\n",
    "    \"Candidate_SemiLeptonic_ST\" : \"f14\",\n",
    "    #\"Candidate_SemiLeptonic_RpT\"\n",
    "    \"EventType_SemiLeptonic\" : \"f15\"\n",
    "}\n",
    "\n",
    "features_l_LepType = {\n",
    "    \"CandidateVBF_Jet_invMass\" : \"f0\",\n",
    "    \"CandidateVBF_Jet_etaSep\" : \"f1\",\n",
    "    \"CandidateVBF_Lead_Jet_pt\" : \"f2\",\n",
    "    \"CandidateVBF_Trail_Jet_pt\" : \"f3\",\n",
    "    \"CandidateHiggs_FatJet_pt\" : \"f4\",\n",
    "    \"CandidateHiggs_FatJet_eta\" : \"f5\",\n",
    "    \"CandidateHiggs_FatJet_msoftdrop\" : \"f6\",\n",
    "    #\"CandidateHiggs_FatJet_particlenetScore\",\n",
    "    \"Candidate_Leptonic_Lead_Lepton_pt\" : \"f7\",\n",
    "    \"Candidate_Leptonic_Trail_Lepton_pt\" : \"f8\",\n",
    "    \"Candidate_Leptonic_Lead_Lepton_eta\" : \"f9\",\n",
    "    \"Candidate_Leptonic_Trail_Lepton_eta\" : \"f10\",\n",
    "    \"Candidate_Leptonic_Lepton_InvMass\" : \"f11\",\n",
    "    \"MET_pt\" : \"f12\",\n",
    "    \"Candidate_Leptonic_ST\" : \"f13\",\n",
    "    \"Lep_dR\" : \"f14\",\n",
    "    \"Lep_dEta\" : \"f15\",\n",
    "    \"Candidate_Leptonic_MT\" : \"f16\",\n",
    "    \"EventType_Leptonic\" : \"f17\"\n",
    "    #\"Candidate_Leptonic_RpT\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add07ecd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bdts_s_LepType = {}\n",
    "bdts_l_LepType = {}\n",
    "eta_s = {\n",
    "        \"16APV\" : 0.464, \n",
    "        \"16\" : 0.543, \n",
    "        \"17\" : 0.478, \n",
    "        \"18\" : 0.732\n",
    "        }\n",
    "\n",
    "n_s = {\n",
    "        \"16APV\" : 1, \n",
    "        \"16\" : 1, \n",
    "        \"17\" : 10, \n",
    "        \"18\" : 6\n",
    "        }\n",
    "\n",
    "mcw_s = {\n",
    "        \"16APV\" : 1, \n",
    "        \"16\" : 1, \n",
    "        \"17\" : 7, \n",
    "        \"18\" : 4\n",
    "        }\n",
    "\n",
    "eta_l = {\n",
    "        \"16APV\" : 0.98, \n",
    "        \"16\" : 0.92, \n",
    "        \"17\" : 0.618, \n",
    "        \"18\" : 0.506\n",
    "        }\n",
    "\n",
    "n_l = {\n",
    "        \"16APV\" : 10, \n",
    "        \"16\" : 6, \n",
    "        \"17\" : 10, \n",
    "        \"18\" : 2\n",
    "        }\n",
    "\n",
    "mcw_l = {\n",
    "        \"16APV\" : 0, \n",
    "        \"16\" : 0, \n",
    "        \"17\" : 0, \n",
    "        \"18\" : 0\n",
    "        }\n",
    "\n",
    "#for sig_name in tqdm.tqdm(signal_xs):\n",
    "#for sig_name in tqdm.tqdm([\"WWH_ucsd_C2V_Reweight\"]):\n",
    "for sig_name in [\"WWH_ucsd_C2V_Reweight\"]:\n",
    "    print(\"TRAINING: \", sig_name)\n",
    "    bdts_s_LepType[sig_name] = {}\n",
    "    bdts_l_LepType[sig_name] = {}\n",
    "\n",
    "    for year in years:\n",
    "        \n",
    "        print(year, \" 1L: \", end=\" \")\n",
    "        bdts_s_LepType[sig_name][year] = btrain.train_bdt(df_combined_s_LepType,features_s_LepType,year,sig_name,n_s[year],eta_s[year],mcw=mcw_s[year],verb=False)\n",
    "        print(year, \" 2L: \", end=\" \")\n",
    "        bdts_l_LepType[sig_name][year] = btrain.train_bdt(df_combined_l_LepType,features_l_LepType,year,sig_name,n_l[year],eta_l[year],mcw=mcw_l[year],verb=False)\n",
    "print()\n",
    "signal_types = [\"C2V_Reweight\"]\n",
    "\n",
    "#for sig_name in tqdm.tqdm(signal_types):\n",
    "for sig_name in signal_types:\n",
    "    print(\"TRAINING: \", signal_types)\n",
    "    bdts_s_LepType[sig_name] = {}\n",
    "    bdts_l_LepType[sig_name] = {}\n",
    "    \n",
    "    for year in years:\n",
    "        \n",
    "        print(year, \" 1L: \", end=\" \")\n",
    "        bdts_s_LepType[sig_name][year] = btrain.train_bdt_mixed(df_combined_s_LepType,features_s_LepType,year,sig_name,n_s[year],eta_s[year],mcw=mcw_s[year],verb=False)\n",
    "        \n",
    "        print(year, \" 2L: \", end=\" \")\n",
    "        bdts_l_LepType[sig_name][year] = btrain.train_bdt_mixed(df_combined_l_LepType,features_l_LepType,year,sig_name,n_l[year],eta_l[year],mcw=mcw_l[year],verb=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5637da",
   "metadata": {},
   "source": [
    "Save BDTs to files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec13986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "for year in years:\n",
    "    ROOT.TMVA.Experimental.SaveXGBoost(bdts_s_LepType[\"C2V_Reweight\"][year], \"bdt_1L_\"+year, \"bdt_1L_\"+year+\".root\")\n",
    "    ROOT.TMVA.Experimental.SaveXGBoost(bdts_l_LepType[\"WWH_ucsd_C2V_Reweight\"][year], \"bdt_2L_\"+year, \"bdt_2L_\"+year+\".root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744958a1",
   "metadata": {},
   "source": [
    "## Check Training Results\n",
    "\n",
    "### Show ranking of variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28555d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_bdt_name_list = [\"C2V_Reweight\",\n",
    "                      \"WWH_ucsd_C2V_Reweight\"\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a86f25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sig_name in full_bdt_name_list:\n",
    "    print(sig_name)\n",
    "    for year in years:\n",
    "        print(year)\n",
    "        xgb.plot_importance(bdts_l_LepType[sig_name][year], importance_type=\"gain\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c740483",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sig_name in full_bdt_name_list:\n",
    "    print(sig_name)\n",
    "    for year in years:\n",
    "        print(year)\n",
    "        xgb.plot_importance(bdts_s_LepType[sig_name][year], importance_type=\"gain\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b57b7c",
   "metadata": {},
   "source": [
    "## Apply Training Results to Samples and Plot\n",
    "\n",
    "### Apply Training Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a33f3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply predictions to WZH and WWH trained together\n",
    "\n",
    "df_SigSampsMixed_l_LepType_dict = {}\n",
    "df_SigSampsMixed_s_LepType_dict = {}\n",
    "\n",
    "features_s_LepType_inv = {v: k for k, v in features_s_LepType.items()}\n",
    "features_l_LepType_inv = {v: k for k, v in features_l_LepType.items()}\n",
    "\n",
    "for sig_name in full_bdt_name_list:\n",
    "    sig_type = sig_name\n",
    "    if \"ucsd\" in sig_type:\n",
    "        sig_type = sig_type[9:]\n",
    "        \n",
    "    \n",
    "    df_SigSampsMixed_l_LepType_dict[sig_name] = {}\n",
    "    df_SigSampsMixed_s_LepType_dict[sig_name] = {}\n",
    "\n",
    "    for year in years:\n",
    "        \n",
    "        df_SigSampsMixed_l_LepType_dict[sig_name][year] = df_combined_l_LepType[(df_combined_l_LepType.year==year) & ((df_combined_l_LepType.sample_isSignal == 0) | (df_combined_l_LepType.sig_type == sig_type))].copy()\n",
    "        df_SigSampsMixed_s_LepType_dict[sig_name][year] = df_combined_s_LepType[(df_combined_s_LepType.year==year) & ((df_combined_s_LepType.sample_isSignal == 0) | (df_combined_s_LepType.sig_type == sig_type))].copy()\n",
    "\n",
    "        df_SigSampsMixed_s_LepType_dict[sig_name][year].rename(columns=features_s_LepType, inplace=True)\n",
    "        df_SigSampsMixed_l_LepType_dict[sig_name][year].rename(columns=features_l_LepType, inplace=True)\n",
    "\n",
    "        \n",
    "        df_SigSampsMixed_l_LepType_dict[sig_name][year][\"bdt\"] = (bdts_l_LepType[sig_name][year].predict_proba(df_SigSampsMixed_l_LepType_dict[sig_name][year][list(features_l_LepType.values())]))[:,1]\n",
    "        df_SigSampsMixed_s_LepType_dict[sig_name][year][\"bdt\"] = (bdts_s_LepType[sig_name][year].predict_proba(df_SigSampsMixed_s_LepType_dict[sig_name][year][list(features_s_LepType.values())]))[:,1]\n",
    "        #display(df_SigSampsMixed_s_LepType_dict[sig_name][year])\n",
    "        \n",
    "        df_SigSampsMixed_s_LepType_dict[sig_name][year].rename(columns=features_s_LepType_inv, inplace=True)\n",
    "        df_SigSampsMixed_l_LepType_dict[sig_name][year].rename(columns=features_l_LepType_inv, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396fd960",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vals = [\"C2V_Reweight\"\n",
    "            ]\n",
    "\n",
    "df_SigSampTrainingCompare_l_LepType_dict = {}\n",
    "df_SigSampTrainingCompare_s_LepType_dict = {}\n",
    "\n",
    "for val in test_vals:\n",
    "    df_SigSampTrainingCompare_l_LepType_dict[val] = {\"WWH\" : pd.concat([df_SigSampsMixed_l_LepType_dict[\"WWH_ucsd_\" + val][\"16\"],\n",
    "                                                               df_SigSampsMixed_l_LepType_dict[\"WWH_ucsd_\" + val][\"17\"],\n",
    "                                                               df_SigSampsMixed_l_LepType_dict[\"WWH_ucsd_\" + val][\"18\"],\n",
    "                                                               df_SigSampsMixed_l_LepType_dict[\"WWH_ucsd_\" + val][\"16APV\"]]),\n",
    "                                                     \"BOTH\" : pd.concat([df_SigSampsMixed_l_LepType_dict[val][\"16\"],\n",
    "                                                               df_SigSampsMixed_l_LepType_dict[val][\"17\"],\n",
    "                                                               df_SigSampsMixed_l_LepType_dict[val][\"18\"],\n",
    "                                                               df_SigSampsMixed_l_LepType_dict[val][\"16APV\"]])}\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_SigSampTrainingCompare_s_LepType_dict[val] = {\"WWH\" : pd.concat([df_SigSampsMixed_s_LepType_dict[\"WWH_ucsd_\" + val][\"16\"],\n",
    "                                                               df_SigSampsMixed_s_LepType_dict[\"WWH_ucsd_\" + val][\"17\"],\n",
    "                                                               df_SigSampsMixed_s_LepType_dict[\"WWH_ucsd_\" + val][\"18\"],\n",
    "                                                               df_SigSampsMixed_s_LepType_dict[\"WWH_ucsd_\" + val][\"16APV\"]]),\n",
    "                                                     \"BOTH\" : pd.concat([df_SigSampsMixed_s_LepType_dict[val][\"16\"],\n",
    "                                                               df_SigSampsMixed_s_LepType_dict[val][\"17\"],\n",
    "                                                               df_SigSampsMixed_s_LepType_dict[val][\"18\"],\n",
    "                                                               df_SigSampsMixed_s_LepType_dict[val][\"16APV\"]])}\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178e5f73",
   "metadata": {},
   "source": [
    "### Show ROC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b4cec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sig_name in full_bdt_name_list:\n",
    "    print(sig_name)\n",
    "    for year in years:\n",
    "        print(year)\n",
    "        df_tmp = df_SigSampsMixed_s_LepType_dict[sig_name][year].copy()\n",
    "        df_train = df_tmp[(df_tmp.split_train) & (df_tmp.year==year)].copy()\n",
    "        df_test = df_tmp[(~df_tmp.split_train) & (df_tmp.year==year)].copy()\n",
    "        \n",
    "        fpr, tpr, thresh = roc_curve(df_train.sample_isSignal, df_train[\"bdt\"])\n",
    "        plt.plot(fpr, tpr, label=\"Training : AUC = {:.2f}\".format(np.trapz(tpr,fpr)));\n",
    "        \n",
    "        fpr, tpr, thresh = roc_curve(df_test.sample_isSignal, df_test[\"bdt\"])\n",
    "        plt.plot(fpr, tpr, label=\"Testing : AUC = {:.2f}\".format(np.trapz(tpr,fpr)));\n",
    "        \n",
    "        plt.xlabel(\"background efficiency\", size=18);\n",
    "        plt.ylabel(\"signal efficiency\", size=18);\n",
    "        plt.legend(fontsize=16);\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sig_name in full_bdt_name_list:\n",
    "    print(sig_name)\n",
    "    for year in years:\n",
    "        print(year)\n",
    "        df_tmp = df_SigSampsMixed_l_LepType_dict[sig_name][year].copy()\n",
    "        df_train = df_tmp[(df_tmp.split_train) & (df_tmp.year==year)].copy()\n",
    "        df_test = df_tmp[(~df_tmp.split_train) & (df_tmp.year==year)].copy()\n",
    "        \n",
    "        fpr, tpr, thresh = roc_curve(df_train.sample_isSignal, df_train[\"bdt\"])\n",
    "        plt.plot(fpr, tpr, label=\"Training : AUC = {:.2f}\".format(np.trapz(tpr,fpr)));\n",
    "        \n",
    "        fpr, tpr, thresh = roc_curve(df_test.sample_isSignal, df_test[\"bdt\"])\n",
    "        plt.plot(fpr, tpr, label=\"Testing : AUC = {:.2f}\".format(np.trapz(tpr,fpr)));\n",
    "        \n",
    "        plt.xlabel(\"background efficiency\", size=18);\n",
    "        plt.ylabel(\"signal efficiency\", size=18);\n",
    "        plt.legend(fontsize=16);\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda41d01",
   "metadata": {},
   "source": [
    "### Draw BDT Scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefa4ea6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bdraw.draw_bdt_score_quant(df_SigSampTrainingCompare_s_LepType_dict[\"C2V_Reweight\"][\"BOTH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dda7d1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bdraw.draw_bdt_score_quant(df_SigSampTrainingCompare_l_LepType_dict[\"C2V_Reweight\"][\"WWH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d5b88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_test_s = df_SigSampTrainingCompare_s_LepType_dict[\"C2V_Reweight\"][\"BOTH\"].copy()\n",
    "\n",
    "df_cut = df_test_s[(df_test_s.CandidateHiggs_FatJet_particlenetScore > 0.9) \n",
    "                                   & (df_test_s.CandidateW_SemiLeptonic_FatJet_particlenetScore > 0.9)\n",
    "                                   & (df_test_s.Candidate_SemiLeptonic_ST > 1000)\n",
    "                                  ].copy()\n",
    "\n",
    "bdraw.draw_bdt_score_quant(df_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca702b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_test_l = (df_SigSampTrainingCompare_l_LepType_dict[\"C2V_Reweight\"][\"WWH\"]).copy()\n",
    "\n",
    "\n",
    "df_cut = df_test_l[(df_test_l.CandidateHiggs_FatJet_particlenetScore > 0.9) \n",
    "                                   & (df_test_l.Candidate_Leptonic_ST > 1000)\n",
    "                                  ].copy()\n",
    "\n",
    "bdraw.draw_bdt_score_quant(df_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54b3e3f",
   "metadata": {},
   "source": [
    "Send values for c2v to be drawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d20fe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bdraw.draw_bdt_score_overlay(df_SigSampTrainingCompare_l_LepType_dict[\"C2V_Reweight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be26db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdraw.draw_bdt_score_overlay(df_SigSampTrainingCompare_s_LepType_dict[\"C2V_Reweight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78713a34",
   "metadata": {},
   "source": [
    "### Draw Variables with BDT Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765fdd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_cut_s = 0.95\n",
    "fontsize=8\n",
    "other_cut = \"\"\n",
    "df_test = df_SigSampTrainingCompare_s_LepType_dict[\"C2V_Reweight\"][\"BOTH\"]\n",
    "\n",
    "df_cut = df_test[(df_test.CandidateHiggs_FatJet_particlenetScore > 0.95) \n",
    "                                   & (df_test.CandidateW_SemiLeptonic_FatJet_particlenetScore > 0.95) \n",
    "                                   & (df_test.CandidateVBF_Jet_etaSep > 5.0)\n",
    "                                  ].copy()\n",
    "#df_cut = df_combined_trained_s_lep[(df_combined_trained_s_lep.CandidateHiggs_FatJet_particlenetScore > 0.9)].copy()\n",
    "\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"CandidateVBF_Jet_invMass\",bdt_cut_s,\"VBF Inv Mass\",np.linspace(0, 5000, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"CandidateVBF_Jet_etaSep\",bdt_cut_s,\"VBF |dEta|\",np.linspace(0, 10, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"CandidateHiggs_FatJet_pt\",bdt_cut_s,\"Higgs PT\",np.linspace(0, 3000, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"CandidateHiggs_FatJet_particlenetScore\",bdt_cut_s,\"Higgs PN Score\",np.linspace(0, 1.0, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"CandidateHiggs_FatJet_mass\",bdt_cut_s,\"Higgs Mass\",np.linspace(0, 500, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"CandidateHiggs_FatJet_msoftdrop\",bdt_cut_s,\"Higgs msoftdrop\",np.linspace(0, 500, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"CandidateW_SemiLeptonic_FatJet_particlenetScore\",bdt_cut_s,\"W PN Score\",np.linspace(0, 1.0, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"CandidateW_SemiLeptonic_FatJet_pt\",bdt_cut_s,\"W PT\",np.linspace(0, 3000, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"CandidateW_SemiLeptonic_FatJet_mass\",bdt_cut_s,\"W Mass\",np.linspace(0, 500, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"CandidateW_SemiLeptonic_FatJet_msoftdrop\",bdt_cut_s,\"W msoftdrop\",np.linspace(0, 500, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"Candidate_SemiLeptonic_Lepton_pt\",bdt_cut_s,\"Lepton PT\",np.linspace(0, 3000, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"MET_pt\",bdt_cut_s,\"MET\",np.linspace(0, 2500, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"Candidate_SemiLeptonic_ST\",bdt_cut_s,\"ST\",np.linspace(0, 3500, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"Candidate_SemiLeptonic_LT\",bdt_cut_s,\"LT\",np.linspace(0, 3500, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"Candidate_SemiLeptonic_RpT\",bdt_cut_s,\"RpT\",np.linspace(0, 5, 101),fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2387a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_cut_l = 0.95\n",
    "fontsize=8\n",
    "other_cut = \"\"\n",
    "df_test = df_SigSampTrainingCompare_l_LepType_dict[\"C2V_Reweight\"][\"WWH\"]\n",
    "\n",
    "df_cut = df_test[(df_test.CandidateHiggs_FatJet_particlenetScore > 0.95) \n",
    "                    & (df_test.CandidateVBF_Jet_etaSep > 5.0)\n",
    "                                  ].copy()\n",
    "#df_cut = df_combined_trained_s_lep[(df_combined_trained_s_lep.CandidateHiggs_FatJet_particlenetScore > 0.9)].copy()\n",
    "\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"CandidateVBF_Jet_invMass\",bdt_cut_s,\"VBF Inv Mass\",np.linspace(0, 5000, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"CandidateVBF_Jet_etaSep\",bdt_cut_s,\"VBF |dEta|\",np.linspace(0, 10, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"CandidateHiggs_FatJet_pt\",bdt_cut_s,\"Higgs PT\",np.linspace(0, 3000, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"CandidateHiggs_FatJet_particlenetScore\",bdt_cut_s,\"Higgs PN Score\",np.linspace(0, 1.0, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"CandidateHiggs_FatJet_mass\",bdt_cut_s,\"Higgs Mass\",np.linspace(0, 500, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"CandidateHiggs_FatJet_msoftdrop\",bdt_cut_s,\"Higgs msoftdrop\",np.linspace(0, 500, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"MET_pt\",bdt_cut_s,\"MET\",np.linspace(0, 2500, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"Candidate_Leptonic_ST\",bdt_cut_s,\"ST\",np.linspace(0, 3500, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"Candidate_Leptonic_LT\",bdt_cut_s,\"LT\",np.linspace(0, 3500, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"Candidate_Leptonic_Lead_Lepton_pt\",bdt_cut_s,\"Lead Lepton PT\",np.linspace(0, 3500, 101),fontsize)\n",
    "bdraw.draw_plots_tmp_2(df_cut,\"Candidate_Leptonic_Trail_Lepton_pt\",bdt_cut_s,\"Trail Lepton PT\",np.linspace(0, 3500, 101),fontsize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e4626",
   "metadata": {},
   "source": [
    "# HYPEROPT SECTION\n",
    "Only needs to be performed once to determine hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab3ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import accuracy_score\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180424f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bdt_mixed_ho(space, df, features, year, signal_type):\n",
    "    if year != \"\":\n",
    "        df_training = df[(df.split_train) & (df.year==year) & ((df.sample_isSignal == 0) | (df.sig_type == signal_type))].sample(frac=1.)\n",
    "        df_testing = df[(~df.split_train) & (df.year==year) & ((df.sample_isSignal == 0) | (df.sig_type == signal_type))].sample(frac=1.)\n",
    "    else:\n",
    "        df_training = df[(df.split_train) & ((df.sample_isSignal == 0) | (df.sig_type == signal_type))].sample(frac=1.)\n",
    "        df_testing = df[(~df.split_train) & ((df.sample_isSignal == 0) | (df.sig_type == signal_type))].sample(frac=1.)\n",
    "\n",
    "    if(len(df_training.index) == 0 or len(df_testing.index) == 0):\n",
    "        print(\"0 occurred: \",year)\n",
    "        return\n",
    "    \n",
    "    X_train, y_train = df_training[features].copy(), df_training.sample_isSignal.values\n",
    "    w_train = (np.abs(df_training.split_weight)).values\n",
    "    X_test, y_test = df_testing[features].copy(), df_testing.sample_isSignal.values\n",
    "    w_test = (np.abs(df_testing.split_weight)).values\n",
    "    \n",
    "    X_train.rename(columns=features, inplace=True)\n",
    "    X_test.rename(columns=features, inplace=True)\n",
    "\n",
    "    xgbd_testing = xgb.DMatrix(\n",
    "        df_testing[features.keys()], \n",
    "        label=df_testing.sample_isSignal,\n",
    "        weight=np.abs(df_testing.split_weight),\n",
    "        nthread=-1\n",
    "    )\n",
    "    xgbd_training = xgb.DMatrix(\n",
    "        df_training[features.keys()], \n",
    "        label=df_training.sample_isSignal, \n",
    "        weight=np.abs(df_training.split_weight),\n",
    "        nthread=-1\n",
    "    )\n",
    "    #evallist = [(xgbd_training, \"train\"), (xgbd_testing, \"eval\")]\n",
    "\n",
    "    nRounds = 500\n",
    "    params = {}\n",
    "    params[\"early_stopping_rounds\"] =  10\n",
    "    params[\"n_estimators\"] =  500\n",
    "    params[\"objective\"] = \"binary:logistic\"\n",
    "    params[\"eval_metric\"] = \"auc\"\n",
    "    params[\"verbosity\"] = 1\n",
    "    params[\"nthread\"] = -1\n",
    "    \n",
    "    params[\"max_depth\"] = int(space['max_depth'])\n",
    "    params[\"learning_rate\"] = space['learning_rate']\n",
    "    params[\"min_child_weight\"] = space['min_child_weight']\n",
    "    #params[\"max_leaves\"] = int(space['max_leaves'])\n",
    "    #params[\"reg_alpha\"] = space['reg_alpha']\n",
    "    #params[\"reg_lambda\"] = space['reg_lambda']\n",
    "\n",
    "    sumWeights_training_signal = np.abs(xgbd_training.get_weight()[xgbd_training.get_label() == 1]).sum()\n",
    "    sumWeights_training_background = np.abs(xgbd_training.get_weight()[xgbd_training.get_label() == 0]).sum()\n",
    "    params[\"scale_pos_weight\"] = sumWeights_training_background/sumWeights_training_signal\n",
    "\n",
    "    xgb_reg = xgb.XGBClassifier(**params)\n",
    "    xgb_reg.fit(X_train, y_train, eval_set=[(X_test, y_test)], sample_weight = w_train, sample_weight_eval_set = [w_test], verbose=False, early_stopping_rounds=10)\n",
    "    #xgb_reg.fit(X_train, y_train, eval_set=[(X_test, y_test)], sample_weight = w_train,  verbose=True)\n",
    "    \n",
    "    pred = (xgb_reg.predict_proba(X_test))[:,1]\n",
    "    accuracy = xgb_reg.best_score#accuracy_score(y_test, pred>0.7)#, sample_weight=w_test)\n",
    "    #print (\"SCORE:\", accuracy)\n",
    "    #change the metric if you like\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK, 'model': xgb_reg}\n",
    "\n",
    "def train_bdt_ho(space, df, features, year, sig_name):\n",
    "            \n",
    "    if year != \"\":\n",
    "        df_training = df[(df.split_train) & (df.year==year) & ((df.sample_isSignal == 0) | (df.sample_name == sig_name))].sample(frac=1.)\n",
    "        df_testing = df[(~df.split_train) & (df.year==year) & ((df.sample_isSignal == 0) | (df.sample_name == sig_name))].sample(frac=1.)\n",
    "    else:\n",
    "        df_training = df[(df.split_train) & ((df.sample_isSignal == 0) | (df.sample_name == sig_name))].sample(frac=1.)\n",
    "        df_testing = df[(~df.split_train) & ((df.sample_isSignal == 0) | (df.sample_name == sig_name))].sample(frac=1.)\n",
    "\n",
    "    split_test_integral = df_testing.xsec_genWeight.sum()\n",
    "    split_train_integral = df_training.xsec_genWeight.sum()\n",
    "    \n",
    "    xgbd_testing = xgb.DMatrix(\n",
    "        df_testing[features.keys()], \n",
    "        label=df_testing.sample_isSignal,\n",
    "        weight=np.abs(df_testing.split_weight),\n",
    "        nthread=-1\n",
    "    )\n",
    "    xgbd_training = xgb.DMatrix(\n",
    "        df_training[features.keys()], \n",
    "        label=df_training.sample_isSignal, \n",
    "        weight=np.abs(df_training.split_weight),\n",
    "        nthread=-1\n",
    "    )\n",
    "    \n",
    "    X_train, y_train = df_training[features].copy(), df_training.sample_isSignal.values\n",
    "    w_train = (np.abs(df_training.split_weight)).values\n",
    "    X_test, y_test = df_testing[features].copy(), df_testing.sample_isSignal.values\n",
    "    w_test = (np.abs(df_testing.split_weight)).values\n",
    "    \n",
    "    X_train.rename(columns=features, inplace=True)\n",
    "    X_test.rename(columns=features, inplace=True)\n",
    "\n",
    "    #display(df_training_l)\n",
    "\n",
    "    #evallist = [(xgbd_training, \"train\"), (xgbd_testing, \"eval\")]\n",
    "\n",
    "    nRounds = 500\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"binary:logistic\"\n",
    "    params[\"eval_metric\"] = \"auc\"\n",
    "    params[\"verbosity\"] = 1\n",
    "    params[\"nthread\"] = -1\n",
    "    \n",
    "    params[\"max_depth\"] = int(space['max_depth'])\n",
    "    params[\"learning_rate\"] = space['learning_rate']\n",
    "    #params[\"gamma\"] = space['gamma']\n",
    "    #params[\"max_leaves\"] = int(space['max_leaves'])\n",
    "    #params[\"reg_alpha\"] = space['reg_alpha']\n",
    "    #params[\"reg_lambda\"] = space['reg_lambda']\n",
    "    params[\"min_child_weight\"] = space['min_child_weight']\n",
    "\n",
    "\n",
    "    sumWeights_training_signal = np.abs(xgbd_training.get_weight()[xgbd_training.get_label() == 1]).sum()\n",
    "    sumWeights_training_background = np.abs(xgbd_training.get_weight()[xgbd_training.get_label() == 0]).sum()\n",
    "    params[\"scale_pos_weight\"] = sumWeights_training_background/sumWeights_training_signal\n",
    "\n",
    "    xgb_reg = xgb.XGBClassifier(**params)\n",
    "    xgb_reg.fit(X_train, y_train, eval_set=[(X_test, y_test)], sample_weight = w_train, sample_weight_eval_set = [w_test], verbose=False, early_stopping_rounds=10)\n",
    "    \n",
    "    pred = (xgb_reg.predict_proba(X_test))[:,1]\n",
    "    accuracy = xgb_reg.best_score#accuracy_score(y_test, pred>0.7)#, sample_weight=w_test)\n",
    "    #print (\"SCORE:\", accuracy)\n",
    "    #change the metric if you like\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK, 'model': xgb_reg}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0b80a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "space={'max_depth': hp.quniform(\"max_depth\", 0, 10, 1),\n",
    "        'learning_rate': hp.uniform ('learning_rate', 0,1),\n",
    "        #'gamma': hp.uniform ('gamma', 1,9),\n",
    "        #'max_leaves' : hp.quniform('max_leaves', 0, 10, 1),\n",
    "        #'reg_alpha' : hp.quniform('reg_alpha', 0,180,1),\n",
    "        #'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ccd618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:46<07:26,  4.86s/trial, best loss: -0.962081]"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    print(year, \": \")\n",
    "    fmin_objective_1L = partial(train_bdt_mixed_ho, df=df_combined_s_LepType, features=features_s_LepType, year=year, signal_type=\"C2V_Reweight\")\n",
    "    fmin_objective_2L = partial(train_bdt_ho, df=df_combined_l_LepType, features=features_l_LepType, year=year, sig_name=\"WWH_ucsd_C2V_Reweight\")\n",
    "\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=fmin_objective_1L,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=100,\n",
    "                trials=trials)\n",
    "\n",
    "    print (best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
